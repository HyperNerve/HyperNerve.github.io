<!DOCTYPE html>
<html>
<head>
  <title>Deep Learning - Data, AI & Machine Learning</title>
  <meta charset="UTF-8">
</head>
<body>

<h1>Deep Learning</h1>
<p>Deep Learning is a subset of Machine Learning that uses multi-layered neural networks to model complex patterns in data. It excels in tasks like image recognition, natural language processing, and speech synthesis.</p>

<h2>1. Key Concepts</h2>
<ul>
  <li><strong>Neural Networks:</strong> Composed of layers of interconnected nodes (neurons)</li>
  <li><strong>Activation Functions:</strong> Introduce non-linearity (e.g., ReLU, Sigmoid)</li>
  <li><strong>Backpropagation:</strong> Algorithm for training by minimizing error</li>
  <li><strong>Gradient Descent:</strong> Optimization technique to update weights</li>
</ul>

<h2>2. Architecture Types</h2>
<ul>
  <li><strong>Feedforward Neural Networks (FNN):</strong> Basic structure with input → hidden → output</li>
  <li><strong>Convolutional Neural Networks (CNN):</strong> Specialized for image data</li>
  <li><strong>Recurrent Neural Networks (RNN):</strong> Handle sequential data (e.g., time series)</li>
  <li><strong>Transformers:</strong> Attention-based models for NLP</li>
</ul>

<h2>3. CNN Example</h2>
<pre>
Task: Image classification
Layers: Convolution → ReLU → Pooling → Fully Connected → Softmax
Output: Label (e.g., "cat", "dog")
</pre>

<h2>4. RNN Example</h2>
<pre>
Task: Sentiment analysis
Input: "The movie was fantastic!"
Output: Positive sentiment
Architecture: RNN or LSTM
</pre>

<h2>5. Transformer Example</h2>
<pre>
Task: Language translation
Input: "Hello" → Output: "Hola"
Model: Transformer (e.g., BERT, GPT)
</pre>

<h2>6. Training Deep Networks</h2>
<ol>
  <li><strong>Initialize weights:</strong> Random or heuristic</li>
  <li><strong>Forward pass:</strong> Compute output from input</li>
  <li><strong>Loss calculation:</strong> Measure prediction error</li>
  <li><strong>Backward pass:</strong> Compute gradients via backpropagation</li>
  <li><strong>Weight update:</strong> Apply gradient descent</li>
</ol>

<h2>7. Common Loss Functions</h2>
<ul>
  <li><strong>Mean Squared Error (MSE):</strong> Regression tasks</li>
  <li><strong>Cross-Entropy:</strong> Classification tasks</li>
  <li><strong>Hinge Loss:</strong> SVM-style classification</li>
</ul>

<h2>8. Tools & Frameworks</h2>
<ul>
  <li><strong>TensorFlow:</strong> Google’s deep learning library</li>
  <li><strong>PyTorch:</strong> Dynamic computation graph, popular in research</li>
  <li><strong>Keras:</strong> High-level API for building models</li>
  <li><strong>ONNX:</strong> Interoperability between frameworks</li>
</ul>

<h2>9. Challenges</h2>
<ul>
  <li><strong>Computational Cost:</strong> Requires GPUs or TPUs</li>
  <li><strong>Overfitting:</strong> Especially with small datasets</li>
  <li><strong>Interpretability:</strong> Hard to explain decisions</li>
  <li><strong>Data Hunger:</strong> Needs large labeled datasets</li>
</ul>

<h2>10. Applications</h2>
<ul>
  <li><strong>Computer Vision:</strong> Object detection, facial recognition</li>
  <li><strong>Natural Language Processing:</strong> Translation, summarization</li>
  <li><strong>Speech:</strong> Voice assistants, transcription</li>
  <li><strong>Gaming:</strong> AI agents in complex environments</li>
</ul>

</body>
    </html>
