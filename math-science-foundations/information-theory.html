<!DOCTYPE html>
<html>
<head>
  <title>Information Theory - Mathematical & Scientific Foundations</title>
  <meta charset="UTF-8">
</head>
<body>

<h1>Information Theory</h1>
<p>Information Theory is the mathematical study of data encoding, transmission, and compression. It quantifies information, analyzes communication systems, and underpins modern technologies like data compression, cryptography, and machine learning.</p>

<h2>1. Core Concepts</h2>
<ul>
  <li><strong>Information:</strong> Reduction in uncertainty from a message</li>
  <li><strong>Entropy (H):</strong> Average information content per symbol</li>
  <li><strong>Mutual Information:</strong> Shared information between variables</li>
  <li><strong>Channel Capacity:</strong> Maximum reliable transmission rate</li>
</ul>

<h2>2. Entropy & Uncertainty</h2>
<ul>
  <li>Shannon entropy: H(X) = −∑ p(x) log₂ p(x)</li>
  <li>Joint entropy: H(X,Y)</li>
  <li>Conditional entropy: H(Y|X)</li>
  <li>Relative entropy (Kullback-Leibler divergence)</li>
</ul>

<h2>3. Source Coding</h2>
<ul>
  <li>Lossless compression: Huffman coding, Arithmetic coding</li>
  <li>Lossy compression: JPEG, MP3, MPEG</li>
  <li>Rate-distortion theory: Trade-off between compression and fidelity</li>
</ul>

<h2>4. Channel Coding</h2>
<ul>
  <li>Error detection: Parity bits, checksums</li>
  <li>Error correction: Hamming codes, Reed-Solomon, LDPC</li>
  <li>Shannon’s noisy channel theorem</li>
</ul>

<h2>5. Communication Models</h2>
<ul>
  <li>Discrete memoryless channels (DMC)</li>
  <li>Binary symmetric channel (BSC)</li>
  <li>Additive white Gaussian noise (AWGN) channel</li>
  <li>Capacity and coding bounds</li>
</ul>

<h2>6. Applications</h2>
<ul>
  <li>Data compression and storage</li>
  <li>Wireless and optical communication</li>
  <li>Cryptography and secure transmission</li>
  <li>Machine learning (feature selection, decision trees)</li>
  <li>Neuroscience and cognitive modeling</li>
</ul>

<h2>7. Programming & Tools</h2>
<ul>
  <li>Python: scikit-learn (mutual_info), NumPy, bitstring</li>
  <li>MATLAB: Communications Toolbox</li>
  <li>R: entropy and infotheo packages</li>
  <li>Custom implementations for coding schemes</li>
</ul>

<h2>8. Challenges</h2>
<ul>
  <li>Modeling real-world noise and interference</li>
  <li>Balancing compression with computational cost</li>
  <li>Designing efficient error-correcting codes</li>
  <li>Extending theory to non-stationary sources</li>
</ul>

<h2>9. Future Directions</h2>
<ul>
  <li>Quantum information theory</li>
  <li>Information-theoretic learning algorithms</li>
  <li>Privacy-preserving data encoding</li>
  <li>Cross-disciplinary applications in biology and cognition</li>
</ul>

</body>
</html>
